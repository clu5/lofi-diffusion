{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "38660de9-529e-49a9-8e61-0ba0e78bcea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import torch\n",
    "import torch.nn.utils.prune as prune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f20f1044-4c23-479d-a4de-378902b9ca9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0809b071e4244c02ad0fe9f934e3c44a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 13 files:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luchar/.pyenv/versions/3.10.8/lib/python3.10/site-packages/transformers/models/clip/feature_extraction_clip.py:28: FutureWarning: The class CLIPFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use CLIPImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cba82dd2d39f4970af6a245d7eadd259",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from diffusers import StableDiffusionPipeline, EulerDiscreteScheduler\n",
    "\n",
    "model_id = \"stabilityai/stable-diffusion-2\"\n",
    "\n",
    "# Use the Euler scheduler here instead\n",
    "scheduler = EulerDiscreteScheduler.from_pretrained(model_id, subfolder=\"scheduler\")\n",
    "pipe = StableDiffusionPipeline.from_pretrained(model_id, scheduler=scheduler, torch_dtype=torch.float16)\n",
    "pipe = pipe.to(\"cuda\")\n",
    "pipe.enable_attention_slicing() # For low GPU RAM\n",
    "\n",
    "prompt = \"a photo of an astronaut riding a horse on mars\"\n",
    "image = pipe(prompt).images[0]\n",
    "    \n",
    "image.save(\"astronaut_rides_horse.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0386b2b2-3dd5-4669-89eb-fb21bd234901",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNET: 865_910_724\n",
      "VAE: 83_653_863\n",
      "CLIP: 340_387_840\n"
     ]
    }
   ],
   "source": [
    "print(f'UNET: {sum(p.numel() for p in pipe.unet.parameters() if p .requires_grad):_}')\n",
    "print(f'VAE: {sum(p.numel() for p in pipe.vae.parameters() if p .requires_grad):_}')\n",
    "print(f'CLIP: {sum(p.numel() for p in pipe.text_encoder.parameters() if p .requires_grad):_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d03fd4-93f6-487b-919a-bf50ec7bc8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for amount in amounts:\n",
    "    print(amount)\n",
    "    lofi_pipe = copy.deepcopy(pipe)\n",
    "    lofi_pipe = lofi_pipe.to(\"cuda\")\n",
    "    unet_params = (\n",
    "        (lofi_pipe.unet.down_blocks[0].resnets[0].conv1, 'weight'),\n",
    "        (lofi_pipe.unet.down_blocks[0].resnets[1].conv1, 'weight'),\n",
    "        (lofi_pipe.unet.down_blocks[1].resnets[0].conv1, 'weight'),\n",
    "        (lofi_pipe.unet.down_blocks[1].resnets[1].conv1, 'weight'),\n",
    "        (lofi_pipe.unet.down_blocks[2].resnets[0].conv1, 'weight'),\n",
    "        (lofi_pipe.unet.down_blocks[2].resnets[1].conv1, 'weight'),\n",
    "        (lofi_pipe.unet.down_blocks[3].resnets[0].conv1, 'weight'),\n",
    "        (lofi_pipe.unet.down_blocks[3].resnets[1].conv1, 'weight'),\n",
    "        (lofi_pipe.unet.up_blocks[0].resnets[0].conv1, 'weight'),\n",
    "        (lofi_pipe.unet.up_blocks[0].resnets[1].conv1, 'weight'),\n",
    "        (lofi_pipe.unet.up_blocks[1].resnets[0].conv1, 'weight'),\n",
    "        (lofi_pipe.unet.up_blocks[1].resnets[1].conv1, 'weight'),\n",
    "        (lofi_pipe.unet.up_blocks[2].resnets[0].conv1, 'weight'),\n",
    "        (lofi_pipe.unet.up_blocks[2].resnets[1].conv1, 'weight'),\n",
    "        (lofi_pipe.unet.up_blocks[3].resnets[0].conv1, 'weight'),\n",
    "        (lofi_pipe.unet.up_blocks[3].resnets[1].conv1, 'weight'),\n",
    "    )\n",
    "    vae_params = (\n",
    "        (lofi_pipe.vae.decoder.up_blocks[0].resnets[0].conv1, 'weight'),\n",
    "        (lofi_pipe.vae.decoder.up_blocks[0].resnets[1].conv1, 'weight'),\n",
    "        (lofi_pipe.vae.decoder.up_blocks[0].resnets[2].conv1, 'weight'),\n",
    "        (lofi_pipe.vae.decoder.up_blocks[1].resnets[0].conv1, 'weight'),\n",
    "        (lofi_pipe.vae.decoder.up_blocks[1].resnets[1].conv1, 'weight'),\n",
    "        (lofi_pipe.vae.decoder.up_blocks[1].resnets[2].conv1, 'weight'),\n",
    "        (lofi_pipe.vae.decoder.up_blocks[2].resnets[0].conv1, 'weight'),\n",
    "        (lofi_pipe.vae.decoder.up_blocks[2].resnets[1].conv1, 'weight'),\n",
    "        (lofi_pipe.vae.decoder.up_blocks[2].resnets[2].conv1, 'weight'),\n",
    "        (lofi_pipe.vae.decoder.up_blocks[3].resnets[0].conv1, 'weight'),\n",
    "        (lofi_pipe.vae.decoder.up_blocks[3].resnets[1].conv1, 'weight'),\n",
    "        (lofi_pipe.vae.decoder.up_blocks[3].resnets[2].conv1, 'weight'),\n",
    "    )\n",
    "\n",
    "    params = unet_params + vae_params\n",
    "    for layer, name in params:\n",
    "        prune.ln_structured(layer, name=name, amount=amount, n=norm, dim=dim)\n",
    "    num_weights = sum(torch.count_nonzero(layer.weight) for layer, name in params)\n",
    "    compression = round((num_weights / total).item(), 2)\n",
    "    print('weights: ', num_weights) \n",
    "    print('compressed: ', compression)\n",
    "  \n",
    "    for prompt in prompts:\n",
    "        generator = torch.Generator(\"cuda\").manual_seed(seed)\n",
    "        images = lofi_pipe([prompt] * 3, num_inference_steps=50, generator=generator, height=512, width=512).images\n",
    "        grid = image_grid(images, rows=3, cols=1)\n",
    "        grid.save(f\"figures/all/compressed-{amount}-{prompt}-{compression}.png\")\n",
    "        plt.figure(figsize=(30, 12))\n",
    "        plt.title(f'prune={amount}\\tcompressed={compression}')\n",
    "        plt.imshow(grid)\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896e2940-bf44-46cd-b5b3-bfa1708c0992",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "def image_grid(imgs, rows, cols):\n",
    "    assert len(imgs) == rows*cols\n",
    "\n",
    "    w, h = imgs[0].size\n",
    "    grid = Image.new('RGB', size=(cols*w, rows*h))\n",
    "    grid_w, grid_h = grid.size\n",
    "    \n",
    "    for i, img in enumerate(imgs):\n",
    "        grid.paste(img, box=(i%cols*w, i//cols*h))\n",
    "    return gri"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
